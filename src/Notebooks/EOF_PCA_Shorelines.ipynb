{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5845815a",
   "metadata": {},
   "source": [
    "# Empirical Orthogonal Function Analysis\n",
    "\n",
    "EOF analysis provides a simple and efficient means of evaluating the dominant modes of variability contained in a dataset. It is done by moving to a new mathematical basis which relates to the covariance of the data, and then focusing the analysis effort on the basis vectors that describe the majority of the variance. It is <u>Principal Component Analysis</u> applied to spatio-temporal data.\n",
    "\n",
    "This code calculates both spatial and temporal eigenvectors (also known as eigenfunctions or EOFs) that describe the variance of a spatio-temporal dataset (e.g. shoreline position data along  a beach). \n",
    "\n",
    "The aim is to describe the datasets variability across both space and time with new basis functions. These are purely mathematic functions (eigenfunctions) but can sometimes relate closely to physical processes. Also, there is some research suggesting a relationship between alongshore and cross-shore transport processes and the shape of the eigenfunctions;\n",
    "- extrema in the spatial eigenfunctions relates to areas of maximum variability,\n",
    "- nodal points in the spatial eigenfunctions relates to areas of stability (low variability),\n",
    "- multiple nodal points in the spatial eigenfunction may indicate the importance of longshore processes,\n",
    "- spatial eigenfunctions without nodes can describe shoreline response to cross-shore processes (i.e. the entire coast advances/retreats in phase).\n",
    "\n",
    "A full positive/negative spatial eigenfunction implies that if erosion (or accretion) is occurring at one location, erosion (or accretion) is also occurring at every other location. Gradients in this eigenfunction can imply a different rate of erosion/accretion.\n",
    "\n",
    "See per Miller & Dean (2006, 2007): Shoreline variability via EOF analysis - https://doi.org/10.1016/j.coastaleng.2006.08.013.\n",
    "This method has been used since by Harley et al. (2011) - https://doi.org/10.1029/2011JF001989 - to show that cross-shore processes are an important mode of variability in beach rotation.\n",
    "\n",
    "Other useful references (EOFs or Principal Component Analysis):\n",
    "- http://stockage.univ-brest.fr/~herbette/Data-Analysis/data_analysis_eof.pdf\n",
    "- https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643\n",
    "- https://www.cygres.com/OcnPageE/Glosry/OcnEof1E.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58be87e",
   "metadata": {},
   "source": [
    "## 1 - Import Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30791729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and modules\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "# System check\n",
    "import sys\n",
    "print('Python version:', sys.version,\n",
    "      '\\nAnaconda environment:', sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251fbe8",
   "metadata": {},
   "source": [
    "## 2 - User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and Check Directory\n",
    "dataDir = 'C:/Users/s5245653/OneDrive - Griffith University/Projects/NaturalShorelineVariability_Grassy/data/'\n",
    "plotDir = 'Plots/All/EOFs/'\n",
    "os.chdir(dataDir)\n",
    "\n",
    "# User inputs\n",
    "resamplePeriod = '1MS' # see Pandas documentation for frequency choices for resampling (e.g. monthly - '1MS')\n",
    "transectSpacing = 50 # metres\n",
    "inputFile = dataDir + 'RSP_raw.csv'\n",
    "idxCol = 'dates' # index column in csv data (needs to be datetime)\n",
    "minVarianceOfInterest = 10 # percent\n",
    "firstDate = datetime.datetime(1987, 9, 1, tzinfo=datetime.timezone.utc) # year, month, day\n",
    "lastDate = datetime.datetime(2020, 12, 31, tzinfo=datetime.timezone.utc)\n",
    "\n",
    "# Plots\n",
    "figSize = [10,6] # figure size in inches\n",
    "resolution = 600 # dpi\n",
    "yTitle = 'EOF' \n",
    "xTitle = 'Alongshore Distance From West (m)'\n",
    "labelLoc = 'upper left'\n",
    "\n",
    "yRange = [-0.55, 0.55] # Spatial EOF\n",
    "yRangeTemp = [-0.199, 0.199] # Temporal EOF\n",
    "plotName = dataDir + plotDir + f'{yTitle}_RSP.jpg'\n",
    "\n",
    "# Second Plot\n",
    "colTitles = ['Spatial', 'Temporal']\n",
    "labelLetters = [['a)', 'b)'], ['c)', 'd)']]\n",
    "plot2Name = dataDir + plotDir + f'{yTitle}_all_RSP.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a0a17",
   "metadata": {},
   "source": [
    "## 3 - Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee08b0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load Raw Data\n",
    "df = pd.read_csv(inputFile, index_col = idxCol)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Create a dataset clean enough for analysis (remove nan values etc.)\n",
    "df2 = df.copy(deep = True)\n",
    "df2 = df2.dropna() # EOF analysis doesn't work with NaN values\n",
    "df2 = df2.truncate(before = firstDate, after = lastDate)\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fa213",
   "metadata": {},
   "source": [
    "## 4 - Compute Eigenfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb95f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create matrix of the data\n",
    "y = df2.to_numpy()\n",
    "y_t = y.transpose()\n",
    "\n",
    "# Create a spatial covariance-like matrix\n",
    "temp = 1 / y.size\n",
    "a = temp * np.matmul(y_t, y) # a measure of the spatial covariance\n",
    "b = temp * np.matmul(y, y_t) # a measure of the temporal covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cbf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors from matrices\n",
    "eigenValues, eigenVectors = np.linalg.eig(a)\n",
    "eigenValuesB, c = np.linalg.eig(b)\n",
    "\n",
    "# Returns eigenValues in order from largest to smallest, with eigenVectors returned as corresponding columns\n",
    "idx = eigenValues.argsort()[::-1]\n",
    "eigenValues = eigenValues[idx]\n",
    "e_x = eigenVectors[:,idx]\n",
    "\n",
    "idx = eigenValuesB.argsort()[::-1]\n",
    "eigenValuesB = eigenValuesB[idx] # first nx eigenvaluesB should equal eigenvalues from A\n",
    "c_t = c[:,idx]\n",
    "\n",
    "# Calculate percent of variance represented by the eigenfunctions\n",
    "pOfVar = np.divide(100 * np.real(eigenValues), np.sum(np.real(eigenValues)))\n",
    "k = sum(map(lambda x : x > minVarianceOfInterest, pOfVar)) # number of eigenfunctions (vectors) to plot\n",
    "# Ensure at least 2 eigenfunctions are plotted\n",
    "if k == 1:\n",
    "    k = 2\n",
    "    \n",
    "# Weights\n",
    "w = [np.sqrt(eigenValues[i]*y.size) for i in range(k)]\n",
    "\n",
    "# Check eigenvalues match between spatial and temporal EOFs\n",
    "np.round(np.real(eigenValuesB[0:min(y.shape)]), 4) == np.round(eigenValues, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8039af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(f'EOF{i+1}: {np.round(pOfVar[i], 2)}%')\n",
    "    \n",
    "print(f'Therefore, the first {k} EOFs describe {np.round(np.sum(pOfVar[0:k]),1)} of the total variance!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a44044",
   "metadata": {},
   "source": [
    "## 5 - Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd454c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Spatial variability\n",
    "\n",
    "# Plot Variables\n",
    "numTransects = len(e_x)\n",
    "xtickLabels = df.columns\n",
    "xticks = np.arange(0, numTransects)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize = figSize, tight_layout = True)\n",
    "\n",
    "for i in range(k):\n",
    "    ax.plot(np.real(e_x[:,i]), label = f'Spatial EOF{i+1} {np.round(pOfVar[i], 2)}%')\n",
    "ax.legend(loc = labelLoc)\n",
    "ax.axhline(0, color = 'k')\n",
    "#ax.grid()\n",
    "\n",
    "plt.xticks(xticks, labels = xtickLabels, rotation = 45)\n",
    "plt.xlabel(xTitle)\n",
    "plt.ylabel(yTitle)\n",
    "plt.ylim(yRange)\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "#plt.savefig(plotName, dpi = resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ae14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = k, ncols = 2, figsize = figSize, tight_layout = True)\n",
    "\n",
    "for i in range(k):\n",
    "    axes[i,0].plot(np.real(e_x[:,i]), label = f'{np.round(pOfVar[i], 2)}%')\n",
    "    axes[i,0].set_ylabel(f'EOF {i+1}')\n",
    "    axes[i,0].set_ylim(yRange)\n",
    "    axes[i,0].legend(loc = labelLoc)\n",
    "    axes[i,1].plot(df2.index, np.real(c_t[:,i]), label = f'{np.round(pOfVar[i], 2)}%')\n",
    "    axes[i,1].set_ylim(yRangeTemp) \n",
    "    for j in range(2):\n",
    "        axes[i,j].autoscale(enable = True, axis ='x', tight = True)\n",
    "        axes[i,j].axhline(0, color = 'k')\n",
    "        axes[i,j].tick_params(direction=\"in\")\n",
    "        axes[i,j].text(-0.1, 0.95, labelLetters[i][j], transform=axes[i,j].transAxes)\n",
    "    axes[i,0].set_xticks([0,5,10,15,20,24])\n",
    "    axes[i,0].set_xticklabels([0, 250, 500, 750, 1000, 1200])\n",
    "\n",
    "# Spatial and Temporal Titles\n",
    "for ax, col in zip(axes[0], colTitles):\n",
    "    ax.set_title(col)\n",
    "axes[1,0].set_xlabel(xTitle)\n",
    "axes[1,1].set_xlabel('Date (yr)')\n",
    "\n",
    "#plt.savefig(plot2Name, dpi = resolution)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25161e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_t1 = np.real(c_t[:,0])\n",
    "c_t2 = np.real(c_t[:,1])\n",
    "c_t3 = np.real(c_t[:,2])\n",
    "\n",
    "df2['c_t1'] = c_t1\n",
    "df2['c_t2'] = c_t2\n",
    "df2['c_t3'] = c_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891589e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtickMonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Monthly averages of \"rotation\" variability\n",
    "fig, ax = plt.subplots(figsize = figSize, tight_layout = True)\n",
    "\n",
    "ax.plot(df2.groupby(df2.index.month)['c_t2'].mean())\n",
    "ax.axhline(0, color = 'k')\n",
    "ax.tick_params(direction=\"in\")\n",
    "ax.autoscale(enable = True, axis ='x', tight = True)\n",
    "plt.xticks(range(1,13), labels = xtickMonths, rotation = 45)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('EOF2 Mean')\n",
    "plt.ylim(-0.075, 0.075)\n",
    "#plt.savefig(dataDir + plotDir + 'rotationEOF.jpg', dpi = resolution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d18170",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9bd0d",
   "metadata": {},
   "source": [
    "## Further Analysis with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMloc = 'C:/Users/s5245653/OneDrive - Griffith University/Projects/GrassyWaves/Data/SAMIndex.csv'\n",
    "SOIloc = 'C:/Users/s5245653/OneDrive - Griffith University/Projects/GrassyWaves/Data/SOI_monthly.txt'\n",
    "\n",
    "\n",
    "SAMI = pd.read_csv(SAMloc, index_col = 'Unnamed: 0', parse_dates = True)\n",
    "SAMI.index = pd.to_datetime(SAMI.index, utc = True)\n",
    "SAMI = SAMI.truncate(before = firstDate, after = lastDate)\n",
    "SAMI.columns = ['SAMI']\n",
    "\n",
    "\n",
    "SOI = pd.read_csv(SOIloc, header = None, index_col = 0, parse_dates = True)\n",
    "SOI.index = pd.to_datetime(SOI.index, format = '%Y%m', utc = True)\n",
    "SOI = SOI.truncate(before = firstDate, after = lastDate)\n",
    "SOI.columns = ['SOI']\n",
    "\n",
    "SAM_SOI = SAMI.join(SOI)\n",
    "SAM_SOI = SAM_SOI.reset_index()\n",
    "SAM_SOI.columns = ['Dates', 'SAMI', 'SOI']\n",
    "SAM_SOI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df2.copy(deep = True)\n",
    "df_ = df_.reset_index()\n",
    "\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    df_,\n",
    "    SAM_SOI[['SAMI', 'SOI']],\n",
    "    left_on=[df_.dates.dt.month, df_.dates.dt.year],\n",
    "    right_on=[SAM_SOI.Dates.dt.month, SAM_SOI.Dates.dt.year],\n",
    ")\n",
    "merged_df.index = merged_df.dates\n",
    "merged_df = merged_df.drop(['key_0', 'key_1', 'dates'], axis = 1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d19c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_winter = SAMI[(SAMI.index.month.isin([6,7,8]))]\n",
    "SAM_winter = SAM_winter.resample('3MS').mean()\n",
    "SAM_winter['winter_to_winter'] = SAM_winter.index.map(lambda x: x.year if x.month >= 1 else x.year-1)\n",
    "SAMposyears = SAM_winter[SAM_winter['SAMI'] >= 0].winter_to_winter\n",
    "SAMnegyears = SAM_winter[SAM_winter['SAMI'] < 0].winter_to_winter\n",
    "\n",
    "merged_df['winteryear'] = merged_df.index.map(lambda x: x.year if x.month >= 1 else x.year-1)\n",
    "\n",
    "\n",
    "df4 = merged_df[merged_df.winteryear.isin(SAMnegyears)]\n",
    "df4 = df4.drop(['c_t1', 'c_t2', 'c_t3', 'SAMI', 'SOI', 'winteryear'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of the data\n",
    "y4 = df4.to_numpy()\n",
    "y4_t = y4.transpose()\n",
    "\n",
    "# Create a spatial covariance-like matrix\n",
    "temp = 1 / y4.size\n",
    "a4 = temp * np.matmul(y4_t, y4) # a measure of the spatial covariance\n",
    "b4 = temp * np.matmul(y4, y4_t) # a measure of the temporal covariance\n",
    "\n",
    "\n",
    "# Compute eigenvalues and eigenvectors from matrices\n",
    "eigenValues4, eigenVectors4 = np.linalg.eig(a4)\n",
    "eigenValuesB4, c4 = np.linalg.eig(b4)\n",
    "\n",
    "# Returns eigenValues in order from largest to smallest, with eigenVectors returned as corresponding columns\n",
    "idx4 = eigenValues4.argsort()[::-1]\n",
    "eigenValues4 = eigenValues4[idx4]\n",
    "e_x4 = eigenVectors4[:,idx4]\n",
    "\n",
    "idx4 = eigenValuesB4.argsort()[::-1]\n",
    "eigenValuesB4 = eigenValuesB4[idx4] # first nx eigenvaluesB should equal eigenvalues from A\n",
    "c_t4 = c4[:,idx4]\n",
    "\n",
    "# Calculate percent of variance represented by the eigenfunctions\n",
    "pOfVar4 = np.divide(100 * np.real(eigenValues4), np.sum(np.real(eigenValues4)))\n",
    "k4 = sum(map(lambda x : x > minVarianceOfInterest, pOfVar4)) # number of eigenfunctions (vectors) to plot\n",
    "# Ensure at least 2 eigenfunctions are plotted\n",
    "if k4 == 1:\n",
    "    k4 = 2\n",
    "    \n",
    "# Weights\n",
    "w4 = [np.sqrt(eigenValues4[i]*y4.size) for i in range(k4)]\n",
    "\n",
    "\n",
    "# Plot Spatial variability\n",
    "\n",
    "# Plot Variables\n",
    "numTransects = len(e_x4)\n",
    "xtickLabels = df4.columns\n",
    "xticks = np.arange(0, numTransects)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize = figSize, tight_layout = True)\n",
    "\n",
    "for i in range(k4):\n",
    "    if i == 0:\n",
    "        ax.plot(np.multiply(-1,np.real(e_x4[:,i])), label = f'Spatial EOF{i+1} {np.round(pOfVar4[i], 2)}%')\n",
    "    else:\n",
    "        ax.plot(np.real(e_x4[:,i]), label = f'Spatial EOF{i+1} {np.round(pOfVar4[i], 2)}%')\n",
    "ax.legend(loc = labelLoc)\n",
    "ax.axhline(0, color = 'k')\n",
    "#ax.grid()\n",
    "\n",
    "plt.xticks(xticks, labels = xtickLabels, rotation = 45)\n",
    "plt.xlabel(xTitle)\n",
    "plt.ylabel(yTitle)\n",
    "plt.ylim(yRange)\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "#plotName = '_pos.jpg'\n",
    "#plt.savefig(plotName, dpi = resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a791159",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(merged_df['SAMI'], merged_df['c_t2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4300d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranges of shoreline positions from the first EOF related to each transect location\n",
    "# e.g. shoreline variability resulting from the first EOF has a range of 19 m at the western profile \n",
    "ranges = []\n",
    "means = []\n",
    "for j in range(k):\n",
    "    temp = [np.ptp((e_x[:,j] * w[j])[i] * c_t1) for i in range(y.shape[1])]\n",
    "    temp2 = [np.mean((e_x[:,j] * w[j])[i] * c_t1) for i in range(y.shape[1])]\n",
    "    ranges.append(temp)\n",
    "    means.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pOfVar[0] + pOfVar[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c363201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
